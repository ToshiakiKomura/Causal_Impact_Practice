group_by(treatment) %>%
summarise(conversion_ratee = mean(conversion),
spend_mean = mean(spend),
count = n())
#t.ttestを使ってt検定を行う(Biased)
##男性向けメールが配信されたグループの購買データを得る
mens_mail_biased <- biased_data %>%
filter(treatment == 1) %>%
pull(spend)
##メールが配信されなかったグループの購買データを得る
no_mail_biased <- biased_data %>%
filter(treatment == 0) %>%
pull(spend)
#平均の差に対して有意差検定を実行
rct_ttest_biased <- t.test(mens_mail_biased, no_mail_biased, var.equal = T)
summary_by_segment_biased
rct_ttest_biased
#バイアスのあるデータでの回帰分析
##回帰分析の実行
biased_reg <- lm(data = biased_data,
formula = spend ~ treatment + history)
##分析結果のレポート
summary(biased_reg)
#ライブラリの読み出し
library("broom")
##推定されたパラメータを取り出す
biased_reg_coef <- tidy(biased_reg)
biased_reg_coef
#RCTデータでの回帰分析とバイアスのあるデータでの回帰分析の比較
##RCTデータでの単回帰
rct_reg <- lm(data=male_df, formula = spend ~ treatment)
rct_reg_coef <- summary(rct_reg)
##バイアスのあるデータでの単回帰
nonrct_reg <- lm(data = biased_data, formula = spend ~ treatment)
nonrct_reg <- summary(nonrct_reg)
rct_reg_coef
nonrct_reg
#バイアスのあるデータでの重回帰
nonrct_mreg <- lm(data = biased_data,
formula = spend ~ treatment + recency + channel + history)
nonrct_mreg_coef <- tidy(nonrct_mreg)
nonrct_mreg_coef
#OVBの確認(broomを利用した場合)
##broomの読み出し
library(broom)
#モデル式のベクトルを用意
formula_vec <-c(spend ~ treatment + recency + channel, #モデルA
spend ~ treatment + recency + channel + history, #モデルB
history~ treatment + channel + recency)#モデルC
##formulaに名前を付ける
names(formula_vec) <- paste("reg", LETTERS[1:3], spe = "_")
#モデル式のデータフレーム化
models <- formula_vec %>%
enframe(name = "model_index", value = "formula")
##まとめて回帰分析を実行
df_models <- models %>%
mutate(model = map(.x = formula, .f = lm, data = biased_data)) %>%
mutate(lm_result = map(.x = model, .f = tidy))
##モデルの結果を整形
df_results <- df_models %>%
mutate(formula = as.character(formula)) %>%
select(formula, model_index, lm_result) %>%
unnest(cols = c(lm_result))
#モデルA,B,Cでのtreatmentのパラメータを抜き出す
treatment_coef <- df_results %>%
filter(term == "treatment") %>%
pull(estimate)
#モデルBからhistoryのパラメータを抜き出す
history_coef <- df_results %>%
filter(model_index == "reg_B",
term == "history") %>%
pull(estimate)
##OVBの確認
OVB <- history_coef*treatment_coef[3]
coef_gap <- treatment_coef[1] - treatment_coef[2]
OVB # beta_2*gamma_1
coef_gap # alpha_1 - beta_1
OVB
coef_gap
#入れてはいけない変数を入れてみる
#visitと介入との相関
cor_visit_treatment <- lm(
data = biased_data,
formula = treatment ~ visit + channel + recency + history) %>%
tidy()
cor_visit_treatment
#visitを入れた回帰分析を実行
bad_control_reg <-  lm(
data = biased_data,
formula = spend ~ treatment + channel + recency + history + visit) %>%
tidy()
bad_control_reg
#傾向スコアの推定
ps_model <- glm(data = biased_data,
formula = treatment ~ recency + history + channel,
family = binomial)
#傾向スコアマッチング
##ライブラリの読み込み
install.packages("MatchIt")
library("MatchIt")
##傾向スコアを利用したマッチング
m_near <- matchit(formula = treatment ~ recency + history + channel,
data = biased_data,
method = "nearest",
replace = TRUE)
##マッチング後のデータを作成
matched_data <- match.data(m_near)
matched_data
install.packages("dplyr")
library(dplyr)
##マッチング後のデータで効果の推定
PSM_result <- matched_data %>%
lm(spend  ~ treatment, data = .)
PSM_result
#逆確率重み付き推定(IPW)
##ライブラリの読み込み
install.packages("WeightIt")
library("WeightIt")
##重みの推定
weighting <- weightit(formula = treatment ~ recency + history + channel,
data = biased_data,
method = "ps",
estimand = "ATE")
##重み付きデータでの効果の推定
IPW_result <- lm(data = biased_data,
formula = spend ~ treatment,
weights = weighting$weights)
IPW_result
#統計モデルを用いたメールの配信ログの分析
##学習データと配信ログを作るデータに分割
set.seed(1)
train_flag <- sample(NROW(male_df), NROW(male_df)/2, replace = FALSE)
male_df_train <- male_df[train_flag,]%>%
filter(treatment == 0)
male_df_test <- male_df[-train_flag,]
##売上が発生する確率を予測するモデルの作成
predict_model <- glm(
data = male_df_train,
formula = conversion ~ recency + history_segment + channel + zip_code,
family = binomial
)
##売上の発生確率からメールの配信確率を決定
pred_cv <- predict(predict_model,
newdata = male_df_test,
type = "response")
pred_cv_rank <- percent_rank(pred_cv)
##配信確率をもとにメールの配信を決定
mail_assign <- sapply(pred_cv_rank, rbinom, n=1, size=1)
##配信ログを作成
ml_male_df <- male_df_test %>%
mutate(mail_assign = mail_assign,
ps = pred_cv_rank) %>%
filter((treatment == 1 & mail_assign ==1)|
(treatment == 1 & mail_assign ==0))
ml_male_df
##実験をしていた場合の平均の差を確認
rct_male_lm <- lm(data = male_df_test, formula = spend ~ treatment)
rct_male_lm
##平均の比較
ml_male_lm <- lm(data = ml_male_df, formula = spend ~ treatment)
ml_male_lm
#Matchingパッケージをインストール
install.packages("Matching")
library(Matching)
PSM_result <- Match(Y = ml_male_df$spend,
Tr = ml_male_df$treatment,
X = ml_male_df$ps,
estimand = "ATT")
PSM_result
##IPWの推定
w.out <- weighit(treatment ~ recency + history _segment + channel + zip_code,
data = ml_male_df
ps = ml_male_df$ps
mathod = "ps"
estimand = "ATE"
)
##IPWの推定
w.out <- weighit(treatment ~ recency + history _segment + channel + zip_code,
data = ml_male_df
ps = ml_male_df$ps
mathod = "ps"
estimand = "ATE"
)
##重み付けしたデータでの共変量のバランスを確認
love.plot(W.out,
threshold = .1)
##重み付けしたデータでの効果の分析
IPW_result <- ml_male_df %>%
lm(data = .,
spend ~ treatment,
weights = W.out$weights)
#havenパッケージのインストール
install.packages("haven")
#ライブラリの読み込み
library("tidyverse")
library("haven")
library("broom")
library("MatchIt")
library("WeightIt")
library("cobalt")
# NBER archiveからデータを読み込む
cps1_data <- read_dta("http://users.nber.org/~rdehejia/data/cps_controls.dta")
cps3_data <- read_dta("http://users.nber.org/~rdehejia/data/cps_controls3.dta")
nswdw_data <- read_dta("http://users.nber.org/~rdehejia/data/nsw_dw.dta")
#データセットの準備
##NSWデータから介入グループだけ取り出してCPS1における介入グループとして扱う
cps1_nsw_data <- nswdw_data %>%
filter(treat == 1) %>%
rbind(cps1_data)
##NSWデータから介入グループだけ取り出してCPS3における介入グループとして扱う
cps3_nsw_data <- nswdw_data %>%
filter(treat == 1) %>%
rbind(cps3_data)
#RCTデータでの分析
##共変量付きの回帰分析
nsw_cov <- nswdw_data %>%
lm(data = .,
re78 ~ treat + re74 + re75  + age + education + black + hispanic + nodegree + married) %>%
filter(term == "treat")
#バイアスのあるデータでの回帰分析
##CPS1の分析結果
cps1_reg <- cps1_nsw_data %>%
lm(data = .,
re78 ~ treat + re74 + re75 + age + education + black + hispanic + nodegree + married)%>%
tidy()%>%
filter(term == "treat")
##CPS3の分析結果
cps3_reg <- cps3_nsw_data %>%
lm(data = .,
re78 ~ treat + re74 + re75 + age + education + black + hispanic + nodegree + married)%>%
tidy()%>%
filter(term == "treat")
cps1_reg
cps3_reg
#傾向スコアマッチングによる効果推定
##傾向スコアを用いたマッチング
m_near <- matchit(treat ~ age + education + black + hispanic + nodegree + married + re74 + re75 + I(re74^2) + I(re75^2),
data = cps1_nsw_data,
method = "nearest")
##共変量のバランスを確認
love.plot(m_near,
threshold = .1)
##マッチング後のデータを作成
matched_data <- match.data(m_near)
##マッチング後のデータで効果の推定
PSM_result_cps1 <- matched_data %>%
lm(re78 ~ treat, data = .)%>%
tidy()
PSM_result_cps1
#IPWによる効果推定
##重みの推定
weighting <- weightit(treat ~ age + education + black + hispanic + nodegree + married +  re74 + re75 + I(re74^2) + I(re75^2),
data = cps1_nsw_data,
method = "ps",
estimand = "ATE")
##共変量のバランスを確認
love.plot(weighting, threshold = .1)
##重み付きデータでの効果の推定
IPW_result <- cps1_nsw_data %>%
lm(data = ., formula = re78 ~ treat,
weights = weighting$weights) %>%
tidy()
IPW_result
library(CausalImpact)
# (2) John Snowデータの読み込み
## Data from Table.12 in Snow(1855)
## http://www.ph.ucla.edu/epi/snow/table12a.html
## 1849年におけるエリア毎のコレラによる死者数
### Southwark and Vauxhall Company
sv1849 <- c(283,157,192,249,259,226,352,97,111,8,235,92)
### Lambeth Company & Southwark and Vauxhall Company
lsv1849 <- c(256,267,312,257,318,446,143,193,243,215,544,187,153,81,113,176)
## 1849年におけるエリア毎のコレラによる死者数
### Southwark and Vauxhall Company
sv1854 <- c(371, 161, 148, 362, 244, 237, 282, 59, 171, 9, 240, 174)
### Lambeth Company & Southwark and Vauxhall Company
lsv1854 <- c(113,174,270,93,210,388,92,58,117,49,193,303,142,48,165,132)
## コレラの死者数を会社ごとにまとめる
sv_death <- c(sv1849, sv1854)
lsv_death <- c(lsv1849, lsv1854)
## どのデータがどのエリアのものか
sv_area <- paste0("sv_",c(1:length(sv1849), 1:length(sv1854)))
lsv_area <- paste0("lsv_", c(1:length(lsv1849), 1:length(lsv1854)))
## どのデータがどの年のものか
sv_year <- c(rep("1849",length(sv1849)), rep("1854", length(sv1854)))
lsv_year <- c(rep("1849",length(lsv1849)), rep("1854", length(lsv1854)))
## Southwark & Vauxhallのデータフレームを作成
sv <- data.frame(area = sv_area,
year = sv_year,
death = sv_death,
LSV = "0",
company = "Southwark and Vauxhall")
## Lambeth & Southwark and Vauxhallのデータフレームを作成
lsv <- data.frame(area = lsv_area,
year = lsv_year,
death = lsv_death,
LSV = "1",
company = "Lambeth & Southwark and Vauxhall")
library(dplyr)
## 地域・年別のデータセットの作成
JS_df <- rbind(sv, lsv) %>%
mutate(LSV =
if_else(company == "Lambeth & Southwark and Vauxhall", 1, 0))
## 会社別のデータセットを作成
JS_sum <- JS_df %>%
group_by(company, LSV, year) %>%
summarise(death = sum(death))
library(tidyverse)
# (3) 集計と可視化による分析
## 集計による推定
JS_grp_summary <- JS_sum %>%
mutate(year = paste("year", year, sep = "_")) %>%
spread(year, death) %>%
mutate(gap = year_1854 - year_1849,
gap_rate = year_1854/year_1849 - 1)
## 集計による推定(log)
JS_grp_summary_ln <- JS_sum %>%
mutate(year = paste("year", year, sep = "_"),
death = log(death)) %>%
spread(year, death) %>%
mutate(gap = year_1854 - year_1849)
## ggplotによる可視化
did_plot <- JS_sum %>%
ggplot(aes(y = death, x = year, shape = company)) +
geom_point(size = 2) +
geom_line(aes(group = company), linetype = 1) +
ylim(2000, 4250) +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
legend.position = "bottom",
plot.margin = margin(1,1,1,1, "cm"))
did_plot
## ggplotによる可視化(アノテーションを追加)
did_plot +
annotate("text", x = 2.2, y = 2400, label = "(1)") +
annotate("text", x = 2.2, y = 3904 + 197*0.6, label = "(2)") +
annotate("text", x = 2.2, y = 3300, label = "(3)") +
annotate("segment", # for common trend in treatment group
x = 1, xend = 2,
y = 3904, yend = 3904 + 197,
arrow = arrow(length = unit(.2,"cm")),
size = 0.1,
linetype = 2) +
annotate("segment", # for parallel trend
x = 1, xend = 2,
y = 2261, yend = 2261,
size = 0.1,
linetype = 2) +
annotate("segment", # for parallel trend
x = 1, xend = 2,
y = 3904, yend = 3904,
size = 0.1,
linetype = 2) +
annotate("segment", # for (1)
x = 2.07, xend = 2.07,
y = 2261, yend = 2458,
arrow = arrow(ends = "both",
length = unit(.1,"cm"),angle = 90)) +
annotate("segment", # for (2)
x = 2.07, xend = 2.07,
y = 3904, yend = 3904 + 197,
arrow = arrow(ends = "both",
length = unit(.1,"cm"),angle = 90)) +
annotate("segment", # for (3)
x = 2.07, xend = 2.07,
y = 3904, yend = 2547,
arrow = arrow(ends = "both",
length = unit(.1,"cm"),angle = 90))
library(tidyr)
# (4) 回帰分析を用いたDID
## Difference in Difference
JS_did <- JS_sum %>%
mutate(D1854 = if_else(year == 1854, 1, 0)) %>%
lm(data = ., death ~ LSV + D1854 + D1854:LSV)
## Difference in Difference(log)
JS_did_log <- JS_sum %>%
mutate(D1854 = if_else(year == 1854, 1, 0)) %>%
lm(data = ., log(death) ~ LSV + D1854 + D1854:LSV)
## Difference in Difference(エリア単位)
JS_did_area <- JS_df %>%
mutate(D1854 = if_else(year == 1854, 1, 0)) %>%
lm(data = ., death ~ LSV + area + D1854 + D1854:LSV) %>%
tidy() %>%
filter(!str_detect(term, "area"))
## Difference in Difference(州単位、log)
JS_did_area_log <- JS_df %>%
mutate(D1854 = if_else(year == 1854, 1, 0)) %>%
lm(data = ., log(death) ~ LSV + area + D1854 + D1854:LSV) %>%
tidy() %>%
filter(!str_detect(term, "area"))
#(5)分析するデータのあるパッケージをインストール(初回のみ)
install.packages("Ecdat")
(6)ライブラリの読み込み
library(Ecdat)
#(7)Proposition99の分析:集計による分析
##データの準備
###Common Trend Assumptionのために分析から特定の州を外す
###タバコの規制を行っていた州のリスト
###Arizona, Oregon, Florida, Massachusetts
###タバコの税金が1988年以降50セント以上上がった州のリスト
###Alaska, Hawaii, Maryland, Michigan, New Jersey, New York, Washington
skip_state <- c(3,9,10,22,21,23,31,33,48)
###Cigarデータセットの読み込み
###skip_stateに含まれる州のデータを除外
Cigar <- Cigar %>%
filter(!state %in% skip_state,
year >=70) %>%
mutate(area = if_else(state ==5, "CA", "Rest of US"))
#(8)DIDのためのデータを準備
##カリフォルニア州とそのほかという2グループのデータ
Cigar_did_sum <- Cigar %>%
mutate(post = if_else(year > 87,1,0),
ca = if_else(state ==5, 1,0),
state = factor(state),
year_dummy = paste("D", year, sep = "_"))%>%
group_by(post, year, year_dummy, ca) %>%
summarise(sales = sum(sales*pop16)/sum(pop16))
library(tidyr)
#(9)カリフォルニア州とそのほかというグループでの分析
##2グループでのデータの分析
Cigar_did_sum_reg <- Cigar_did_sum %>%
lm(data = ., sales ~ ca+post+ca:post +year_dummy)%>%
tidy()%>%
filter(!str_detect(term, "state"),
!str_detect(term, "year"))
##2グループでのデータでの分析(log)
Cigar_did_sum_logreg <- Cigar_did_sum %>%
lm(data =., log(sales) ~ca + post + ca:post + year_dummy) %>%
tidy() %>%
filter(!str_detect(term, "state"),
!str_detect(term, "year"))
#(11)Causal Impactを利用した分析
##ライブラリのインストール(初回のみ)
install.packages("CausalImpact")
##CigarデータをCausalImpact用に整形
###目的変数としてカリフォルニア州の売上だけ抜き出す
Y <- Cigar %>% filter(state ==5) %>% pull(sales)
###共変量として他の州の売上を抜き出し整形
X_sales <- Cigar %>%
filter(state !=5)%>%
select(state, sales, year)%>%
spread(state,sales)
###介入が行われるデータを示す
pred_oeriod <- c(1:NROW(X_sales))[X_sales$year < 88]
post_period <- c(1:NROW(X_sales))[X_sales$year >=88]
###目的変数と共変量をバインド
CI_data <- cbind(Y,X_sales) %>% select(-year)
##Causal Impactによる分析
impact <- CausalImpact::CausalImpact(CI_data,
pre.period = c(min(pre_period), max(pre.period)),
post.period = c(min(post_period), max(post_period)))
##結果のplot
plot(impact)
#(1)ライブラリの読み出し
library("tidyverse")
library("broom")
#(2)データの読み込み
email_data <- read_csv("http://www.minethatdata.com/Kevin_Hillstrom_MineThatData_E-MailAnalytics_DataMiningChallenge_2008.03.20.csv")
#(3)ルールによるメールの配信を行ったログを作成
##データの整形とrunning varialeの追加
male_data <- email_data %>%
filter(segment %in% c("Mens E-Mail", "No-E-Mail"))%>%
mutate(treatment = if_else(segment == "Mens E-Mail", 1, 0),
history_log = log(history))
##cut-offの値を指定
threshold_value <- 5.5
##ルールによる介入を再現したデータの作成
##cut-offによりrunninbg variableが大きければ配信されたデータのみ残す
##逆の場合は配信されなかったデータのみ残す
##running variableを0.1単位で区切ったグループ分けの変数を追加
rdd_data  <- male_data %>%
mutate(history_log_grp = round(history_log/0.1,0)*0.1)  %>%
filter(((history_log > threshold_value)&
(segment == "Mens E-Mail")) |
(history_log <= threshold_value)&
(segment == "No E-Mail"))
##RDDデータでの比較
rdd_data_table <- rdd_data %>%
group_by(treatment)%>%
summarise(count= n(),
visit_rate = mean(visit))
#(6)回帰分析による分析
##線形回帰による分析
rdd_lm_reg <- rdd_data %>%
mutate(treatment = if_else(segment == "Mens E-Mail", 1, 0))%>%
lm(data = ., formula = visit ~ treatment +  history_log) %>%
tidy() %>%
filter(term == "treatment")
## 非線形回帰による分析
library("rddtools")
nonlinear_rdd_data <- rdd_data(y = rdd_data$visit,
x = rdd_data$history_log,
cutpoint = 5.5)
nonlinear_rdd_ord4 <- rdd_reg_lm(rdd_object=nonlinear_rdd_data, order=4)
nonlinear_rdd_ord4
#(8)nonparametric RDD
##ライブラリの読み込み
install.packages("rdd")
library("rdd")
##non-parametric RDDの実行
rdd_result <- RDestimate(data  = rdd_data,
formula = visit ~ history_log,
cutpoint = 5.5)
##結果のレポート
summary(rdd_result)
##結果のプロット
plot(rdd_result)
